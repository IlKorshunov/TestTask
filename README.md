# Прогнозирование толщины элементов резервуаров

## Структура проекта

Проект организован следующим образом:

- `src/` - основной код проекта
  - `main.py` - главный скрипт, запускающий весь пайплайн
  - `data_processor.py` - обработка и парсинг исходных данных из CSV.
  - `feature_selector.py` - отбор наиболее важных признаков
  - `model_trainer.py` - обучение различных моделей машинного обучения
  - `hyperparameter_tuner.py` - подбор гиперпараметров моделей
  - `analyze_results.py` - анализ результатов и создание отчетов
  - `myPlot.py` - визуализация данных
  - `config.py` - настройки проекта
- `data/` - данные проекта
  - исходные CSV файлы (tank.csv, element.csv, ElementData.csv, Activity.csv)
  - обработанные данные (processed_data.csv, final_data.csv)
  - аугментированные данные (augmented_data.csv, при условии, что уагментация была)
- `models/` - сохраненные обученные модели
- `tests/` - тесты для основных компонентов
- `plots/` - сохраненные графики и визуализации

## Установка и запуск

### Требования

При реализации использовался 3.9. Все зависимости перечислены в файле `requirements.txt`.

### Установка зависимостей


```bash
python3 -m venv venv
source venv/bin/activate 
pip install -r requirements.txt
cd src
python main.py
python analyze_results.py
python myPlot.py
```


Последовательность действий:
1. Загружает и обрабатывает исходные данные из CSV файлов
2. Парсит JSON
3. Join таблиц
4. Создание и получение признаков
5. Отбор важных
6. Аугментация
7. Обучение нескольких моделей и подбор лучше


## Как работает проект

### Обработка данных

Исходные данные приходят в виде CSV файлов, где некоторые поля содержат JSON объекты. Класс `DataProcessor` парсит эти JSON объекты и извлекает из них структурированные признаки.

Например, из объекта резервуара извлекаются такие параметры как диаметр, год начала эксплуатации, тип резервуара, характеристики стали, информация о нагрузках и грунте. Из данных об элементах извлекается информация о сварных швах и методах контроля. Из измерений - толщина, дефекты, даты измерений.

После парсинга данные объединяются по связям между таблицами. Для каждого измерения добавляется информация о соответствующем элементе, резервуаре и активности.

### Создание признаков

Из временных меток создаются признаки: год, месяц, квартал измерения, возраст резервуара на момент измерения. Также вычисляются признаки, связанные с историей измерений элемента: предыдущее значение толщины, скорость коррозии, количество дней с последнего измерения.

Агрегируются статистики по элементам: средняя толщина, стандартное отклонение, минимальное и максимальное значение, количество измерений за время эксплуатации.

### Отбор признаков

Класс `FeatureSelector` отбирает наиболее информативные признаки. Сначала удаляются признаки с низкой дисперсией (менее 5%), затем удаляются сильно коррелированные признаки (коэффициент корреляции выше 0.75), и наконец выбираются топ-N признаков по важности (по умолчанию 10).


### Обучение моделей

Проект обучает несколько типов моделей:

- Линейная регрессия (базовая модель)
- Ridge и Lasso регрессия (с регуляризацией)
- Случайный лес (Random Forest)
- Градиентный бустинг (Gradient Boosting)
- Дерево решений
- Нейронная сеть с механизмом внимания (DeepRegressionNet)

Для каждой модели вычисляются метрики: средняя квадратичная ошибка (MSE), средняя абсолютная ошибка (MAE) и коэффициент детерминации (R2). Модель с лучшими метриками сохраняется в файл `models/best_model.joblib`.

Если данных достаточно (более 500 записей), автоматически запускается подбор гиперпараметров для лучших моделей с помощью GridSearchCV.

### Предобработка данных

Перед обучением категориальные признаки кодируются через OHE, а числовые стандартизируются к N(0,1).

## Результаты

После выполнения пайплайна в папке `models/` сохраняется лучшая модель. В папке `data/` сохраняются обработанные данные:

- `processed_data.csv` - данные после парсинга и объединения
- `final_data.csv` - финальный датасет после отбора признаков и аугментации
- `augmented_data.csv` - аугментированные данные (если применялась аугментация)

Логи выполнения выводятся в консоль и показывают прогресс обработки, размеры данных на каждом этапе и метрики моделей.

### Визуализации

В папке `plots/` сохраняются графики анализа данных:

**Обработанные данные:**
- `01_processed_overview.png` - общий обзор обработанных данных
- `02_processed_delta.png` - распределение целевой переменной delta
- `03_processed_correlation.png` - корреляционная матрица признаков

**Финальный датасет:**
- `04_final_overview.png` - обзор финального датасета после отбора признаков
- `05_final_delta.png` - распределение delta после аугментации
- `06_feature_selection_comparison.png` - сравнение методов отбора признаков

Графики можно сгенерировать запуском:
```bash
cd src
python myPlot.py
```

## Тестирование

```bash
python -m pytest tests/
```

## Настройки

Основные настройки проекта находятся в файле `src/config.py`. Там можно изменить:

- Пути к данным и моделям
- Целевую переменную (по умолчанию "delta")
- Количество отбираемых признаков (по умолчанию 10)
- Размер тестовой выборки (по умолчанию 20%)
- Параметры случайного состояния для воспроизводимости

## Особенности реализации

При работе с данными учитывалось, что JSON поля могут быть пустыми или некорректными, поэтому парсинг выполняется с обработкой ошибок. Пропущенные значения обрабатываются корректно на всех этапах.

Временные признаки создаются с учетом того, что измерения могут быть нерегулярными. Скорость коррозии вычисляется только если есть предыдущее измерение для того же элемента.


## Возможные улучшения

В будущем можно добавить:

- Дополнительные временные признаки (сезонность, тренды, генерация новых признаков)
- Ансамбль моделей
- Мониторинг качества предсказаний в реальном времени (Tensorboard)